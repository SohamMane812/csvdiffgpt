{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from csvdiffgpt import summarize\n",
    "from csvdiffgpt import compare\n",
    "import os\n",
    "from csvdiffgpt import restructure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "API_KEY = os.getenv(\"GEMINI_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Okay, let's analyze the differences between the two baseball datasets.\n",
      "\n",
      "**1. Overview**\n",
      "\n",
      "Both datasets contain baseball player statistics. The first dataset has 771 rows and 16 columns, while the second has 809 rows and 17 columns. The key difference is the addition of an 'AVG' column in the second dataset and an increase in the number of rows.\n",
      "\n",
      "**2. Structure Changes**\n",
      "\n",
      "*   **Added Column:** The second dataset includes a new column named 'AVG' (Batting Average), which is of float64 type.\n",
      "*   **Row Count:** The second dataset has 38 more rows than the first (809 vs. 771), representing a 4.93% increase.\n",
      "\n",
      "**3. Content Changes**\n",
      "\n",
      "*   The values in the common columns ('G', 'Age', 'BB', 'AB', 'H', 'RBI', '3B', '2B', 'CS', 'R', 'SB', 'SO', 'HR', 'PA') have not changed.\n",
      "\n",
      "**4. Statistical Changes**\n",
      "\n",
      "*   The mean of column 'G' changed from 66.2 to 66.44.\n",
      "*   The mean of column 'PA' changed from 208.64 to 209.83.\n",
      "*   The mean of column 'AB' changed from 185.61 to 186.7.\n",
      "*   The mean of column 'R' changed from 24.05 to 24.22.\n",
      "*   The mean of column 'H' changed from 47.83 to 48.23.\n",
      "*   The mean of column '2B' changed from 8.44 to 8.52.\n",
      "*   The mean of column 'HR' changed from 4.95 to 5.01.\n",
      "*   The mean of column 'RBI' changed from 22.56 to 22.76.\n",
      "*   The mean of column 'SB' changed from 4.3 to 4.21.\n",
      "*   The mean of column 'CS' changed from 2.1 to 2.09.\n",
      "*   The mean of column 'BB' changed from 18.45 to 18.49.\n",
      "*   The mean of column 'SO' changed from 32.04 to 32.11.\n",
      "*   The distribution of values in column '3B' changed slightly.\n",
      "\n",
      "**5. Recommendations**\n",
      "\n",
      "*   The addition of the 'AVG' column in the second dataset is a significant enhancement, providing a key performance indicator directly in the data.\n",
      "*   The increase in row count suggests that more player data has been included in the updated dataset.\n",
      "*   The changes in the means of the columns are very small, so they are probably not significant.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "result = compare(\n",
    "    file1=\"./baseball.csv\",\n",
    "    file2=\"./baseball2.csv\",\n",
    "    question=\"What changed between these versions?\",\n",
    "    api_key=API_KEY,\n",
    "    provider=\"gemini\",\n",
    "    model='gemini-2.0-flash'\n",
    ")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'file1': {'path': './baseball.csv', 'row_count': 771, 'column_count': 16, 'metadata': {'file_path': './baseball.csv', 'file_size_mb': 0.04, 'separator': ',', 'total_rows': 771, 'total_columns': 16, 'analyzed_rows': 771, 'analyzed_columns': 16, 'columns': {'Last': {'type': 'object', 'nulls': 0, 'null_percentage': np.float64(0.0), 'unique_count': 657, 'min_length': 3, 'max_length': 14, 'avg_length': 6.52, 'examples': ['Lansford', 'Lowry', 'Beckwith', 'Mullins', 'Valle']}, 'First': {'type': 'object', 'nulls': 0, 'null_percentage': np.float64(0.0), 'unique_count': 273, 'min_length': 1, 'max_length': 9, 'avg_length': 4.35, 'examples': ['Keith', 'Jay', 'Rafael', 'Tom', 'Rick']}, 'Age': {'type': 'int64', 'nulls': 0, 'null_percentage': np.float64(0.0), 'unique_count': 24, 'min': np.int64(20), 'max': np.int64(45), 'mean': 27.98, 'median': 27.0, 'std': 4.37, 'examples': [22, 32, 26, 21, 37]}, 'G': {'type': 'int64', 'nulls': 0, 'null_percentage': np.float64(0.0), 'unique_count': 160, 'min': np.int64(1), 'max': np.int64(163), 'mean': 66.2, 'median': 56.0, 'std': 52.16, 'examples': [138, 1, 117, 144, 107]}, 'PA': {'type': 'int64', 'nulls': 0, 'null_percentage': np.float64(0.0), 'unique_count': 373, 'min': np.int64(0), 'max': np.int64(742), 'mean': 208.64, 'median': 101.0, 'std': 224.02, 'examples': [158, 308, 641, 14, 237]}, 'AB': {'type': 'int64', 'nulls': 0, 'null_percentage': np.float64(0.0), 'unique_count': 365, 'min': np.int64(0), 'max': np.int64(687), 'mean': 185.61, 'median': 90.0, 'std': 199.51, 'examples': [20, 510, 0, 217, 23]}, 'R': {'type': 'int64', 'nulls': 0, 'null_percentage': np.float64(0.0), 'unique_count': 103, 'min': np.int64(0), 'max': np.int64(130), 'mean': 24.05, 'median': 9.0, 'std': 29.16, 'examples': [48, 0, 48, 1, 26]}, 'H': {'type': 'int64', 'nulls': 0, 'null_percentage': np.float64(0.0), 'unique_count': 169, 'min': np.int64(0), 'max': np.int64(238), 'mean': 47.83, 'median': 19.0, 'std': 55.67, 'examples': [0, 2, 101, 0, 45]}, '2B': {'type': 'int64', 'nulls': 0, 'null_percentage': np.float64(0.0), 'unique_count': 44, 'min': np.int64(0), 'max': np.int64(53), 'mean': 8.44, 'median': 3.0, 'std': 10.46, 'examples': [19, 1, 5, 4, 27]}, '3B': {'type': 'int64', 'nulls': 0, 'null_percentage': np.float64(0.0), 'unique_count': 14, 'min': np.int64(0), 'max': np.int64(14), 'mean': 1.11, 'median': 0.0, 'std': 1.93, 'examples': [1, 0, 7, 1, 2], 'value_distribution': {'0': 60.57, '1': 13.88, '2': 8.56, '3': 5.97, '4': 4.15, '5': 2.33, '6': 1.82, '7': 1.69, '9': 0.26, '10': 0.26}}, 'HR': {'type': 'int64', 'nulls': 0, 'null_percentage': np.float64(0.0), 'unique_count': 37, 'min': np.int64(0), 'max': np.int64(40), 'mean': 4.95, 'median': 1.0, 'std': 7.72, 'examples': [2, 0, 0, 0, 0]}, 'RBI': {'type': 'int64', 'nulls': 0, 'null_percentage': np.float64(0.0), 'unique_count': 108, 'min': np.int64(0), 'max': np.int64(121), 'mean': 22.56, 'median': 8.0, 'std': 28.29, 'examples': [1, 36, 0, 0, 55]}, 'SB': {'type': 'int64', 'nulls': 0, 'null_percentage': np.float64(0.0), 'unique_count': 46, 'min': np.int64(0), 'max': np.int64(107), 'mean': 4.3, 'median': 0.0, 'std': 9.87, 'examples': [2, 4, 0, 5, 1]}, 'CS': {'type': 'int64', 'nulls': 0, 'null_percentage': np.float64(0.0), 'unique_count': 20, 'min': np.int64(0), 'max': np.int64(19), 'mean': 2.1, 'median': 0.0, 'std': 3.52, 'examples': [0, 2, 0, 1, 0]}, 'BB': {'type': 'int64', 'nulls': 0, 'null_percentage': np.float64(0.0), 'unique_count': 92, 'min': np.int64(0), 'max': np.int64(105), 'mean': 18.45, 'median': 7.0, 'std': 23.02, 'examples': [0, 0, 0, 3, 5]}, 'SO': {'type': 'int64', 'nulls': 0, 'null_percentage': np.float64(0.0), 'unique_count': 126, 'min': np.int64(0), 'max': np.int64(185), 'mean': 32.04, 'median': 20.0, 'std': 34.59, 'examples': [4, 5, 4, 0, 21]}}, 'sample_provided': False}}, 'file2': {'path': './baseball2.csv', 'row_count': 809, 'column_count': 17, 'metadata': {'file_path': './baseball2.csv', 'file_size_mb': 0.05, 'separator': ',', 'total_rows': 809, 'total_columns': 17, 'analyzed_rows': 809, 'analyzed_columns': 17, 'columns': {'Last': {'type': 'object', 'nulls': 0, 'null_percentage': np.float64(0.0), 'unique_count': 657, 'min_length': 3, 'max_length': 14, 'avg_length': 6.52, 'examples': ['Kennedy', 'Boggs', 'Moore', 'L Washington', 'Ashby']}, 'First': {'type': 'object', 'nulls': 0, 'null_percentage': np.float64(0.0), 'unique_count': 273, 'min_length': 1, 'max_length': 9, 'avg_length': 4.36, 'examples': ['Mike', 'Ray', 'Dennis', 'Charlie', 'Dave']}, 'Age': {'type': 'int64', 'nulls': 0, 'null_percentage': np.float64(0.0), 'unique_count': 24, 'min': np.int64(20), 'max': np.int64(45), 'mean': 27.98, 'median': 27.0, 'std': 4.37, 'examples': [31, 28, 33, 23, 38]}, 'G': {'type': 'int64', 'nulls': 0, 'null_percentage': np.float64(0.0), 'unique_count': 160, 'min': np.int64(1), 'max': np.int64(163), 'mean': 66.44, 'median': 56.0, 'std': 52.39, 'examples': [1, 5, 61, 151, 149]}, 'PA': {'type': 'int64', 'nulls': 0, 'null_percentage': np.float64(0.0), 'unique_count': 373, 'min': np.int64(0), 'max': np.int64(742), 'mean': 209.83, 'median': 102.0, 'std': 225.73, 'examples': [187, 700, 640, 208, 41]}, 'AB': {'type': 'int64', 'nulls': 0, 'null_percentage': np.float64(0.0), 'unique_count': 365, 'min': np.int64(0), 'max': np.int64(687), 'mean': 186.7, 'median': 91.0, 'std': 201.09, 'examples': [0, 379, 101, 0, 542]}, 'R': {'type': 'int64', 'nulls': 0, 'null_percentage': np.float64(0.0), 'unique_count': 103, 'min': np.int64(0), 'max': np.int64(130), 'mean': 24.22, 'median': 9.0, 'std': 29.46, 'examples': [0, 1, 45, 76, 82]}, 'H': {'type': 'int64', 'nulls': 0, 'null_percentage': np.float64(0.0), 'unique_count': 169, 'min': np.int64(0), 'max': np.int64(238), 'mean': 48.23, 'median': 20.0, 'std': 56.23, 'examples': [3, 41, 0, 0, 0]}, '2B': {'type': 'int64', 'nulls': 0, 'null_percentage': np.float64(0.0), 'unique_count': 44, 'min': np.int64(0), 'max': np.int64(53), 'mean': 8.52, 'median': 3.0, 'std': 10.57, 'examples': [1, 0, 0, 0, 0]}, '3B': {'type': 'int64', 'nulls': 0, 'null_percentage': np.float64(0.0), 'unique_count': 14, 'min': np.int64(0), 'max': np.int64(14), 'mean': 1.1, 'median': 0.0, 'std': 1.92, 'examples': [1, 1, 0, 0, 5], 'value_distribution': {'0': 60.94, '1': 13.6, '2': 8.65, '3': 5.81, '4': 4.2, '5': 2.35, '6': 1.85, '7': 1.61, '9': 0.25, '10': 0.25}}, 'HR': {'type': 'int64', 'nulls': 0, 'null_percentage': np.float64(0.0), 'unique_count': 37, 'min': np.int64(0), 'max': np.int64(40), 'mean': 5.01, 'median': 1.0, 'std': 7.89, 'examples': [0, 0, 1, 20, 0]}, 'RBI': {'type': 'int64', 'nulls': 0, 'null_percentage': np.float64(0.0), 'unique_count': 108, 'min': np.int64(0), 'max': np.int64(121), 'mean': 22.76, 'median': 8.0, 'std': 28.64, 'examples': [5, 0, 0, 31, 1]}, 'SB': {'type': 'int64', 'nulls': 0, 'null_percentage': np.float64(0.0), 'unique_count': 46, 'min': np.int64(0), 'max': np.int64(107), 'mean': 4.21, 'median': 0.0, 'std': 9.71, 'examples': [2, 8, 0, 0, 0]}, 'CS': {'type': 'int64', 'nulls': 0, 'null_percentage': np.float64(0.0), 'unique_count': 20, 'min': np.int64(0), 'max': np.int64(19), 'mean': 2.09, 'median': 0.0, 'std': 3.5, 'examples': [2, 0, 0, 0, 0]}, 'BB': {'type': 'int64', 'nulls': 0, 'null_percentage': np.float64(0.0), 'unique_count': 92, 'min': np.int64(0), 'max': np.int64(105), 'mean': 18.49, 'median': 7.0, 'std': 22.99, 'examples': [48, 22, 62, 0, 3]}, 'SO': {'type': 'int64', 'nulls': 0, 'null_percentage': np.float64(0.0), 'unique_count': 126, 'min': np.int64(0), 'max': np.int64(185), 'mean': 32.11, 'median': 20.0, 'std': 34.83, 'examples': [2, 8, 30, 11, 2]}, 'AVG': {'type': 'float64', 'nulls': 0, 'null_percentage': np.float64(0.0), 'unique_count': 531, 'min': np.float64(0.076923077), 'max': np.float64(172.3846154), 'mean': 48.44, 'median': 24.08, 'std': 51.11, 'examples': [88.69230769, 41.53846154, 4.615384615, 6.769230769, 19.69230769]}}, 'sample_provided': False}}, 'comparison': {'structural_changes': {'common_columns': ['G', 'Age', 'First', 'BB', 'AB', 'H', 'RBI', 'Last', '3B', '2B', 'CS', 'R', 'SB', 'SO', 'HR', 'PA'], 'only_in_file1': [], 'only_in_file2': ['AVG'], 'type_changes': {}, 'row_count_change': {'file1': 771, 'file2': 809, 'difference': 38, 'percent_change': 4.93}}, 'value_changes': {'G': {'mean_abs_diff': 0.0, 'max_abs_diff': 0.0, 'diff_count': 0, 'diff_percentage': 0.0}, 'Age': {'mean_abs_diff': 0.0, 'max_abs_diff': 0.0, 'diff_count': 0, 'diff_percentage': 0.0}, 'BB': {'mean_abs_diff': 0.0, 'max_abs_diff': 0.0, 'diff_count': 0, 'diff_percentage': 0.0}, 'AB': {'mean_abs_diff': 0.0, 'max_abs_diff': 0.0, 'diff_count': 0, 'diff_percentage': 0.0}, 'H': {'mean_abs_diff': 0.0, 'max_abs_diff': 0.0, 'diff_count': 0, 'diff_percentage': 0.0}, 'RBI': {'mean_abs_diff': 0.0, 'max_abs_diff': 0.0, 'diff_count': 0, 'diff_percentage': 0.0}, '3B': {'mean_abs_diff': 0.0, 'max_abs_diff': 0.0, 'diff_count': 0, 'diff_percentage': 0.0}, '2B': {'mean_abs_diff': 0.0, 'max_abs_diff': 0.0, 'diff_count': 0, 'diff_percentage': 0.0}, 'CS': {'mean_abs_diff': 0.0, 'max_abs_diff': 0.0, 'diff_count': 0, 'diff_percentage': 0.0}, 'R': {'mean_abs_diff': 0.0, 'max_abs_diff': 0.0, 'diff_count': 0, 'diff_percentage': 0.0}, 'SB': {'mean_abs_diff': 0.0, 'max_abs_diff': 0.0, 'diff_count': 0, 'diff_percentage': 0.0}, 'SO': {'mean_abs_diff': 0.0, 'max_abs_diff': 0.0, 'diff_count': 0, 'diff_percentage': 0.0}, 'HR': {'mean_abs_diff': 0.0, 'max_abs_diff': 0.0, 'diff_count': 0, 'diff_percentage': 0.0}, 'PA': {'mean_abs_diff': 0.0, 'max_abs_diff': 0.0, 'diff_count': 0, 'diff_percentage': 0.0}}}}\n"
     ]
    }
   ],
   "source": [
    "# Using compare() without LLM\n",
    "comparison_data = compare(\n",
    "    file1=\"./baseball.csv\",\n",
    "    file2=\"./baseball2.csv\",\n",
    "    use_llm=False\n",
    ")\n",
    "print(comparison_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Okay, I will analyze the provided baseball dataset metadata and provide a summary.\n",
      "\n",
      "**1. Overview**\n",
      "\n",
      "This dataset contains baseball player statistics, with 771 rows (representing individual players) and 16 columns (representing various statistics and personal information). The file size is relatively small at 0.04 MB. The data includes batting statistics such as At Bats (AB), Runs (R), Hits (H), Home Runs (HR), and Strikeouts (SO), as well as personal information like name and age.\n",
      "\n",
      "**2. Key Variables**\n",
      "\n",
      "*   **Last & First:** Player's last and first names. These are object (string) types with no missing values. The last names have a higher unique count (657) than first names (273), as expected.\n",
      "*   **Age:** Player's age, ranging from 20 to 45 years old, with an average age of approximately 28.\n",
      "*   **G:** Games played, ranging from 1 to 163.\n",
      "*   **PA:** Plate Appearances, ranging from 0 to 742.\n",
      "*   **AB:** At Bats, ranging from 0 to 687.\n",
      "*   **R:** Runs scored, ranging from 0 to 130.\n",
      "*   **H:** Hits, ranging from 0 to 238.\n",
      "*   **HR:** Home Runs, ranging from 0 to 40.\n",
      "*   **RBI:** Runs Batted In, ranging from 0 to 121.\n",
      "*   **SB:** Stolen Bases, ranging from 0 to 107.\n",
      "*   **SO:** Strikeouts, ranging from 0 to 185.\n",
      "\n",
      "**3. Data Quality**\n",
      "\n",
      "*   **Missing Values:** The metadata indicates that there are no missing values (nulls) in any of the columns.\n",
      "*   **Outliers:** Given the ranges of values for statistics like HR, RBI, SB, and SO, it's possible that some players have outlier values. Further analysis would be needed to confirm this. The large standard deviations for several variables (e.g., G, PA, AB, R, H, HR, RBI, SB, SO) suggest a wide range of performance among the players in the dataset.\n",
      "*   **Data Types:** All columns appear to have appropriate data types assigned.\n",
      "\n",
      "**4. Patterns & Insights**\n",
      "\n",
      "*   **Experience vs. Performance:** It would be interesting to analyze the relationship between Age and other performance metrics (e.g., HR, RBI, H). We could investigate if there's a peak performance age.\n",
      "*   **Stolen Base Attempts:** The data includes both Stolen Bases (SB) and Caught Stealing (CS). The ratio of SB to (SB + CS) could be calculated to determine a player's stolen base success rate.\n",
      "*   **Offensive Production:** Several variables (H, 2B, 3B, HR, RBI, R) are related to offensive production. These could be combined or analyzed together to create a more comprehensive offensive performance metric.\n",
      "*   **Strikeout Rate:** The ratio of Strikeouts (SO) to At Bats (AB) could be calculated to determine a player's strikeout rate.\n",
      "\n",
      "**5. Recommendations**\n",
      "\n",
      "*   **Explore Relationships:** Conduct correlation analysis to identify relationships between different variables. For example, explore the correlation between AB and H, or HR and RBI.\n",
      "*   **Outlier Analysis:** Investigate potential outliers in key performance metrics to understand if they represent exceptional players or data errors.\n",
      "*   **Feature Engineering:** Create new features from existing ones to gain further insights. Examples include:\n",
      "    *   Batting Average (H / AB)\n",
      "    *   On-Base Percentage (OBP)\n",
      "    *   Slugging Percentage (SLG)\n",
      "    *   Stolen Base Success Rate (SB / (SB + CS))\n",
      "*   **Data Visualization:** Use histograms, scatter plots, and box plots to visualize the distributions of variables and relationships between them.\n",
      "*   **Advanced Analytics:** Consider more advanced statistical techniques, such as regression analysis, to model and predict player performance.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "result = summarize(\n",
    "    \"./baseball.csv\",\n",
    "    question=\"What insights can you provide about this dataset?\",\n",
    "    api_key=API_KEY,\n",
    "    provider=\"gemini\",\n",
    "    model='gemini-2.0-flash'\n",
    ")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'file_path': './baseball.csv', 'file_size_mb': 0.04, 'separator': ',', 'total_rows': 771, 'total_columns': 16, 'analyzed_rows': 771, 'analyzed_columns': 16, 'columns': {'Last': {'type': 'object', 'nulls': 0, 'null_percentage': np.float64(0.0), 'unique_count': 657, 'min_length': 3, 'max_length': 14, 'avg_length': 6.52, 'examples': ['Griffin', 'Booker', 'McCullers', 'Barfield', 'Reuschel']}, 'First': {'type': 'object', 'nulls': 0, 'null_percentage': np.float64(0.0), 'unique_count': 273, 'min_length': 1, 'max_length': 9, 'avg_length': 4.35, 'examples': ['Tony', 'Juan', 'Floyd', 'Paul', 'Luis']}, 'Age': {'type': 'int64', 'nulls': 0, 'null_percentage': np.float64(0.0), 'unique_count': 24, 'min': np.int64(20), 'max': np.int64(45), 'mean': 27.98, 'median': 27.0, 'std': 4.37, 'examples': [26, 33, 31, 31, 39]}, 'G': {'type': 'int64', 'nulls': 0, 'null_percentage': np.float64(0.0), 'unique_count': 160, 'min': np.int64(1), 'max': np.int64(163), 'mean': 66.2, 'median': 56.0, 'std': 52.16, 'examples': [145, 93, 126, 132, 13]}, 'PA': {'type': 'int64', 'nulls': 0, 'null_percentage': np.float64(0.0), 'unique_count': 373, 'min': np.int64(0), 'max': np.int64(742), 'mean': 208.64, 'median': 101.0, 'std': 224.02, 'examples': [19, 299, 264, 562, 178]}, 'AB': {'type': 'int64', 'nulls': 0, 'null_percentage': np.float64(0.0), 'unique_count': 365, 'min': np.int64(0), 'max': np.int64(687), 'mean': 185.61, 'median': 90.0, 'std': 199.51, 'examples': [299, 399, 11, 5, 6]}, 'R': {'type': 'int64', 'nulls': 0, 'null_percentage': np.float64(0.0), 'unique_count': 103, 'min': np.int64(0), 'max': np.int64(130), 'mean': 24.05, 'median': 9.0, 'std': 29.16, 'examples': [98, 15, 57, 12, 1]}, 'H': {'type': 'int64', 'nulls': 0, 'null_percentage': np.float64(0.0), 'unique_count': 169, 'min': np.int64(0), 'max': np.int64(238), 'mean': 47.83, 'median': 19.0, 'std': 55.67, 'examples': [103, 0, 154, 118, 0]}, '2B': {'type': 'int64', 'nulls': 0, 'null_percentage': np.float64(0.0), 'unique_count': 44, 'min': np.int64(0), 'max': np.int64(53), 'mean': 8.44, 'median': 3.0, 'std': 10.46, 'examples': [0, 6, 0, 1, 3]}, '3B': {'type': 'int64', 'nulls': 0, 'null_percentage': np.float64(0.0), 'unique_count': 14, 'min': np.int64(0), 'max': np.int64(14), 'mean': 1.11, 'median': 0.0, 'std': 1.93, 'examples': [0, 0, 0, 0, 0], 'value_distribution': {'0': 60.57, '1': 13.88, '2': 8.56, '3': 5.97, '4': 4.15, '5': 2.33, '6': 1.82, '7': 1.69, '9': 0.26, '10': 0.26}}, 'HR': {'type': 'int64', 'nulls': 0, 'null_percentage': np.float64(0.0), 'unique_count': 37, 'min': np.int64(0), 'max': np.int64(40), 'mean': 4.95, 'median': 1.0, 'std': 7.72, 'examples': [16, 1, 4, 9, 0]}, 'RBI': {'type': 'int64', 'nulls': 0, 'null_percentage': np.float64(0.0), 'unique_count': 108, 'min': np.int64(0), 'max': np.int64(121), 'mean': 22.56, 'median': 8.0, 'std': 28.29, 'examples': [35, 32, 13, 1, 23]}, 'SB': {'type': 'int64', 'nulls': 0, 'null_percentage': np.float64(0.0), 'unique_count': 46, 'min': np.int64(0), 'max': np.int64(107), 'mean': 4.3, 'median': 0.0, 'std': 9.87, 'examples': [20, 0, 2, 2, 0]}, 'CS': {'type': 'int64', 'nulls': 0, 'null_percentage': np.float64(0.0), 'unique_count': 20, 'min': np.int64(0), 'max': np.int64(19), 'mean': 2.1, 'median': 0.0, 'std': 3.52, 'examples': [1, 0, 6, 0, 1]}, 'BB': {'type': 'int64', 'nulls': 0, 'null_percentage': np.float64(0.0), 'unique_count': 92, 'min': np.int64(0), 'max': np.int64(105), 'mean': 18.45, 'median': 7.0, 'std': 23.02, 'examples': [21, 15, 13, 13, 15]}, 'SO': {'type': 'int64', 'nulls': 0, 'null_percentage': np.float64(0.0), 'unique_count': 126, 'min': np.int64(0), 'max': np.int64(185), 'mean': 32.04, 'median': 20.0, 'std': 34.59, 'examples': [74, 2, 23, 59, 79]}}, 'sample_provided': False}\n"
     ]
    }
   ],
   "source": [
    "metadata = summarize(\n",
    "    file=\"./baseball.csv\",\n",
    "    use_llm=False\n",
    ")\n",
    "print(metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "```text\n",
      "## Data Quality Validation Report: Baseball Dataset\n",
      "\n",
      "### 1. Data Quality Summary\n",
      "The dataset appears to be relatively clean, with no missing values, high cardinality issues, inconsistent values, or type issues reported. However, there are a significant number of outliers identified across several numeric columns, which could potentially skew analysis and modeling results. The outlier severity is rated as \"low\" to \"medium,\" suggesting that while present, they may not be extreme.\n",
      "\n",
      "### 2. Missing Values\n",
      "No missing values were reported in the metadata. This is a positive aspect of the data quality.\n",
      "\n",
      "### 3. Outliers\n",
      "The following columns have identified outliers:\n",
      "*   **Age:** 2 outliers (0.26%) - Low severity\n",
      "*   **R (Runs):** 3 outliers (0.39%) - Low severity\n",
      "*   **H (Hits):** 2 outliers (0.26%) - Low severity\n",
      "*   **2B (Doubles):** 4 outliers (0.52%) - Low severity\n",
      "*   **3B (Triples):** 21 outliers (2.72%) - Medium severity\n",
      "*   **HR (Home Runs):** 18 outliers (2.33%) - Medium severity\n",
      "*   **RBI (Runs Batted In):** 9 outliers (1.17%) - Medium severity\n",
      "*   **SB (Stolen Bases):** 17 outliers (2.2%) - Medium severity\n",
      "*   **CS (Caught Stealing):** 20 outliers (2.59%) - Medium severity\n",
      "*   **BB (Walks):** 10 outliers (1.3%) - Medium severity\n",
      "*   **SO (Strikeouts):** 11 outliers (1.43%) - Medium severity\n",
      "\n",
      "The outlier percentages range from 0.26% to 2.72%. Columns with higher outlier percentages (3B, HR, SB, CS) warrant closer inspection. The impact of these outliers depends on the specific analysis being performed. For example, if you are building a model to predict a player's salary, outliers in performance metrics like HR or RBI could disproportionately influence the model.\n",
      "\n",
      "### 4. Inconsistencies\n",
      "No inconsistencies were reported in the metadata.\n",
      "\n",
      "### 5. Recommendations\n",
      "\n",
      "1.  **Investigate Outliers:**\n",
      "    *   **Severity Focus:** Prioritize investigating outliers in columns with \"medium\" severity and higher outlier percentages (3B, HR, SB, CS, RBI, SO, BB).\n",
      "    *   **Domain Knowledge:** Use baseball domain knowledge to determine if the outlier values are legitimate exceptional performances or data errors. For example, a very high SB value might be valid for a speed-focused player.\n",
      "    *   **Visualization:** Visualize the distributions of the columns with outliers (e.g., box plots, histograms) to better understand the nature and extent of the outliers.\n",
      "\n",
      "2.  **Outlier Handling:**\n",
      "    *   **Justification:** Document the rationale for any outlier handling decisions.\n",
      "    *   **Options:** Consider the following options for handling outliers:\n",
      "        *   **Winsorizing/Capping:** Limit extreme values to a specified percentile.\n",
      "        *   **Transformation:** Apply transformations (e.g., log transformation) to reduce the impact of outliers.\n",
      "        *   **Removal:** Remove outliers only if they are confirmed to be data errors and their removal is justifiable.\n",
      "        *   **Keep as is:** If the outliers represent genuine extreme values, consider keeping them, especially if they provide valuable information.\n",
      "\n",
      "3.  **Consider Interaction Effects:**\n",
      "    *   Explore whether outliers in one column are related to values in other columns. For example, a high HR value might be more significant for a player with a low SO value.\n",
      "\n",
      "4.  **Re-evaluate Outlier Detection Method:**\n",
      "    *   Understand the method used to detect outliers. The metadata does not specify the method. Common methods include using standard deviations, interquartile ranges (IQR), or domain-specific rules. Ensure the method is appropriate for the data and the analysis goals.\n",
      "\n",
      "5.  **Data Understanding:**\n",
      "    *   Gain a deeper understanding of the data collection process and the meaning of each column. This will help in identifying potential data quality issues that are not immediately apparent from the metadata.\n",
      "```\n"
     ]
    }
   ],
   "source": [
    "from csvdiffgpt import validate\n",
    "\n",
    "# With LLM\n",
    "result = validate(\"./baseball.csv\", api_key=API_KEY, model='gemini-2.0-flash')\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 11 total issues\n",
      "- Missing values: 0 columns\n",
      "- Outliers: 11 columns\n",
      "- High cardinality: 0 columns\n",
      "- Inconsistent values: 0 columns\n",
      "- Type issues: 0 columns\n",
      "\n",
      "Outlier issues:\n",
      "- Column 'Age': 2 outliers (0.25%), severity: low\n",
      "- Column 'R': 3 outliers (0.37%), severity: low\n",
      "- Column 'H': 2 outliers (0.25%), severity: low\n",
      "- Column '2B': 4 outliers (0.49%), severity: low\n",
      "- Column '3B': 21 outliers (2.6%), severity: medium\n",
      "- Column 'HR': 22 outliers (2.72%), severity: medium\n",
      "- Column 'RBI': 6 outliers (0.74%), severity: low\n",
      "- Column 'SB': 17 outliers (2.1%), severity: medium\n",
      "- Column 'CS': 21 outliers (2.6%), severity: medium\n",
      "- Column 'BB': 10 outliers (1.24%), severity: medium\n",
      "- Column 'SO': 13 outliers (1.61%), severity: medium\n"
     ]
    }
   ],
   "source": [
    "from csvdiffgpt import validate\n",
    "import json\n",
    "\n",
    "# Without LLM\n",
    "validation_data = validate(\"./baseball2.csv\", use_llm=False)\n",
    "\n",
    "# Print summary\n",
    "print(f\"Found {validation_data['summary']['total_issues']} total issues\")\n",
    "print(f\"- Missing values: {validation_data['summary']['missing_values_columns']} columns\")\n",
    "print(f\"- Outliers: {validation_data['summary']['outlier_columns']} columns\")\n",
    "print(f\"- High cardinality: {validation_data['summary']['high_cardinality_columns']} columns\")\n",
    "print(f\"- Inconsistent values: {validation_data['summary']['inconsistent_columns']} columns\")\n",
    "print(f\"- Type issues: {validation_data['summary']['type_issue_columns']} columns\")\n",
    "\n",
    "# Print detailed missing value issues\n",
    "if validation_data['issues']['missing_values']:\n",
    "    print(\"\\nMissing value issues:\")\n",
    "    for issue in validation_data['issues']['missing_values']:\n",
    "        print(f\"- Column '{issue['column']}': {issue['null_percentage']}% null, severity: {issue['severity']}\")\n",
    "\n",
    "# Print detailed outlier issues\n",
    "if validation_data['issues']['outliers']:\n",
    "    print(\"\\nOutlier issues:\")\n",
    "    for issue in validation_data['issues']['outliers']:\n",
    "        print(f\"- Column '{issue['column']}': {issue['outlier_count']} outliers ({issue['outlier_percentage']}%), severity: {issue['severity']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Data cleaning script\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "\n",
      "# Load the CSV file\n",
      "df = pd.read_csv('baseball.csv', sep=',')\n",
      "\n",
      "# Apply cleaning steps\n",
      "\n",
      "# Step 1: cap_outliers for Age\n",
      "# Cap outliers based on z-score\n",
      "z_scores = np.abs((df['Age'] - df['Age'].mean()) / df['Age'].std())\n",
      "outlier_mask = z_scores > 3.0\n",
      "df.loc[outlier_mask, 'Age'] = np.sign(df.loc[outlier_mask, 'Age'] - df['Age'].mean()) * 3.0 * df['Age'].std() + df['Age'].mean()\n",
      "\n",
      "# Step 2: cap_outliers for R\n",
      "# Cap outliers based on z-score\n",
      "z_scores = np.abs((df['R'] - df['R'].mean()) / df['R'].std())\n",
      "outlier_mask = z_scores > 3.0\n",
      "df.loc[outlier_mask, 'R'] = np.sign(df.loc[outlier_mask, 'R'] - df['R'].mean()) * 3.0 * df['R'].std() + df['R'].mean()\n",
      "\n",
      "# Step 3: cap_outliers for H\n",
      "# Cap outliers based on z-score\n",
      "z_scores = np.abs((df['H'] - df['H'].mean()) / df['H'].std())\n",
      "outlier_mask = z_scores > 3.0\n",
      "df.loc[outlier_mask, 'H'] = np.sign(df.loc[outlier_mask, 'H'] - df['H'].mean()) * 3.0 * df['H'].std() + df['H'].mean()\n",
      "\n",
      "# Step 4: cap_outliers for 2B\n",
      "# Cap outliers based on z-score\n",
      "z_scores = np.abs((df['2B'] - df['2B'].mean()) / df['2B'].std())\n",
      "outlier_mask = z_scores > 3.0\n",
      "df.loc[outlier_mask, '2B'] = np.sign(df.loc[outlier_mask, '2B'] - df['2B'].mean()) * 3.0 * df['2B'].std() + df['2B'].mean()\n",
      "\n",
      "# Step 5: cap_outliers for 3B\n",
      "# Cap outliers based on z-score\n",
      "z_scores = np.abs((df['3B'] - df['3B'].mean()) / df['3B'].std())\n",
      "outlier_mask = z_scores > 3.0\n",
      "df.loc[outlier_mask, '3B'] = np.sign(df.loc[outlier_mask, '3B'] - df['3B'].mean()) * 3.0 * df['3B'].std() + df['3B'].mean()\n",
      "\n",
      "# Step 6: cap_outliers for HR\n",
      "# Cap outliers based on z-score\n",
      "z_scores = np.abs((df['HR'] - df['HR'].mean()) / df['HR'].std())\n",
      "outlier_mask = z_scores > 3.0\n",
      "df.loc[outlier_mask, 'HR'] = np.sign(df.loc[outlier_mask, 'HR'] - df['HR'].mean()) * 3.0 * df['HR'].std() + df['HR'].mean()\n",
      "\n",
      "# Step 7: cap_outliers for RBI\n",
      "# Cap outliers based on z-score\n",
      "z_scores = np.abs((df['RBI'] - df['RBI'].mean()) / df['RBI'].std())\n",
      "outlier_mask = z_scores > 3.0\n",
      "df.loc[outlier_mask, 'RBI'] = np.sign(df.loc[outlier_mask, 'RBI'] - df['RBI'].mean()) * 3.0 * df['RBI'].std() + df['RBI'].mean()\n",
      "\n",
      "# Step 8: cap_outliers for SB\n",
      "# Cap outliers based on z-score\n",
      "z_scores = np.abs((df['SB'] - df['SB'].mean()) / df['SB'].std())\n",
      "outlier_mask = z_scores > 3.0\n",
      "df.loc[outlier_mask, 'SB'] = np.sign(df.loc[outlier_mask, 'SB'] - df['SB'].mean()) * 3.0 * df['SB'].std() + df['SB'].mean()\n",
      "\n",
      "# Step 9: cap_outliers for CS\n",
      "# Cap outliers based on z-score\n",
      "z_scores = np.abs((df['CS'] - df['CS'].mean()) / df['CS'].std())\n",
      "outlier_mask = z_scores > 3.0\n",
      "df.loc[outlier_mask, 'CS'] = np.sign(df.loc[outlier_mask, 'CS'] - df['CS'].mean()) * 3.0 * df['CS'].std() + df['CS'].mean()\n",
      "\n",
      "# Step 10: cap_outliers for BB\n",
      "# Cap outliers based on z-score\n",
      "z_scores = np.abs((df['BB'] - df['BB'].mean()) / df['BB'].std())\n",
      "outlier_mask = z_scores > 3.0\n",
      "df.loc[outlier_mask, 'BB'] = np.sign(df.loc[outlier_mask, 'BB'] - df['BB'].mean()) * 3.0 * df['BB'].std() + df['BB'].mean()\n",
      "\n",
      "# Step 11: cap_outliers for SO\n",
      "# Cap outliers based on z-score\n",
      "z_scores = np.abs((df['SO'] - df['SO'].mean()) / df['SO'].std())\n",
      "outlier_mask = z_scores > 3.0\n",
      "df.loc[outlier_mask, 'SO'] = np.sign(df.loc[outlier_mask, 'SO'] - df['SO'].mean()) * 3.0 * df['SO'].std() + df['SO'].mean()\n",
      "\n",
      "# Save cleaned data\n",
      "df.to_csv('baseball_cleaned.csv', index=False)\n"
     ]
    }
   ],
   "source": [
    "from csvdiffgpt import clean\n",
    "result = clean(\"./baseball.csv\", use_llm=False)\n",
    "print(result[\"sample_code\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Okay, I've analyzed the baseball dataset metadata and validation results. Here's a plan to clean the data:\n",
      "\n",
      "## 1. Data Quality Assessment\n",
      "\n",
      "The dataset appears to be relatively clean, with no missing values or type issues. The primary concern is the presence of outliers in several numerical columns. While the outlier percentages are generally low, some columns like '3B', 'HR', 'SB', and 'CS' have a higher proportion of outliers. These outliers could skew analysis and modeling results.\n",
      "\n",
      "## 2. Recommended Cleaning Steps\n",
      "\n",
      "Here's a prioritized list of cleaning steps:\n",
      "\n",
      "**Priority 1: Handling Outliers**\n",
      "\n",
      "*   **Strategy:** Winsorizing is a good approach for handling outliers in this dataset. It replaces extreme values with less extreme values, preserving the overall data distribution better than simply removing them. We'll use a winsorizing approach, capping values at the 1st and 99th percentiles.\n",
      "*   **Columns:** Apply winsorizing to the following columns: 'Age', 'R', 'H', '2B', '3B', 'HR', 'RBI', 'SB', 'CS', 'BB', and 'SO'.\n",
      "\n",
      "**Priority 2: Name Standardization (Optional)**\n",
      "\n",
      "*   **Strategy:** While not strictly a data *cleaning* issue, standardizing names can be helpful for analysis. This involves removing leading/trailing whitespace and ensuring consistent capitalization.\n",
      "*   **Columns:** 'First', 'Last'\n",
      "\n",
      "## 3. Python Code Examples\n",
      "\n",
      "Here's how to implement the winsorizing and name standardization using pandas:\n",
      "\n",
      "```python\n",
      "import pandas as pd\n",
      "from scipy.stats import winsorize\n",
      "\n",
      "# Load the dataset\n",
      "df = pd.read_csv(\"baseball.csv\")\n",
      "\n",
      "# Winsorize numerical columns\n",
      "columns_to_winsorize = ['Age', 'R', 'H', '2B', '3B', 'HR', 'RBI', 'SB', 'CS', 'BB', 'SO']\n",
      "\n",
      "for col in columns_to_winsorize:\n",
      "    df[col] = winsorize(df[col], limits=[0.01, 0.01])  # Cap at 1st and 99th percentiles\n",
      "\n",
      "# Standardize names\n",
      "df['First'] = df['First'].str.strip().str.title()\n",
      "df['Last'] = df['Last'].str.strip().str.title()\n",
      "\n",
      "# Display the cleaned data (first 5 rows)\n",
      "print(df.head())\n",
      "```\n",
      "\n",
      "## 4. Additional Considerations\n",
      "\n",
      "*   **Data Loss:** Winsorizing does not remove data, but it does alter the values of outliers. The choice of percentile limits (1% and 99%) is a balance between reducing the impact of outliers and preserving the original data distribution. You might need to adjust these limits based on your specific analysis goals.\n",
      "*   **Alternative Outlier Handling:** Other outlier handling methods exist, such as removing outliers entirely or using more sophisticated statistical techniques. However, winsorizing is a good starting point for this dataset due to its simplicity and data preservation.\n",
      "*   **Domain Knowledge:** Consider consulting with baseball experts to understand if certain \"outliers\" are actually valid and important data points. For example, a player with an exceptionally high number of stolen bases might be a legitimate star player, not an error.\n",
      "*   **Iterative Cleaning:** Data cleaning is often an iterative process. After applying these steps, re-evaluate the data quality and adjust the cleaning process as needed.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from csvdiffgpt.tasks.clean import clean\n",
    "\n",
    "# Using the clean function with OpenAI\n",
    "result = clean(\n",
    "    file=\"./baseball.csv\",\n",
    "    question=\"What are the best approaches to clean this baseball dataset?\",\n",
    "    api_key=API_KEY,  \n",
    "    provider=\"gemini\",\n",
    "    model=\"gemini-2.0-flash\"  \n",
    ")\n",
    "\n",
    "# Print the LLM's recommendations\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "```sql\n",
      "## Schema Assessment\n",
      "\n",
      "The current schema consists of a single table with player statistics. The data types seem appropriate for the values they hold. However, there's a significant amount of redundancy due to highly correlated columns. This redundancy can lead to increased storage costs and potentially slower query performance.  The lack of a primary key is also a concern.\n",
      "\n",
      "## Recommended Schema Changes\n",
      "\n",
      "1.  **Add a Primary Key:** Create a primary key column to uniquely identify each player.  Since there's no existing unique identifier, we'll create a surrogate key.\n",
      "\n",
      "2.  **Address Redundancy:**  Analyze and potentially remove highly correlated columns. The analysis suggests several columns are highly correlated: PA, AB, R, and H.  It's crucial to understand the business context before removing any column.  If these stats are always tracked together, keeping them might be acceptable. However, if one can be reliably derived from another, removing the redundant column is beneficial.\n",
      "\n",
      "3.  **Consider Normalization (Optional):** If the dataset grows significantly and includes more player-related information (e.g., biographical details, team affiliations), consider normalizing the schema by creating separate tables for players and statistics. This would reduce data duplication and improve data integrity.  However, given the current size and scope, this might be premature optimization.\n",
      "\n",
      "4.  **Indexing:** Add indexes to columns frequently used in queries, such as `Last`, `First`, and `Age`.\n",
      "\n",
      "## Implementation Approach\n",
      "\n",
      "1.  **Create a New Table (with Primary Key):** Create a new table with an auto-incrementing primary key and the desired columns.\n",
      "\n",
      "2.  **Data Migration:** Migrate the data from the old table to the new table.\n",
      "\n",
      "3.  **Drop Redundant Columns (Optional):** After careful analysis, drop the redundant columns from the new table.\n",
      "\n",
      "4.  **Create Indexes:** Create indexes on frequently queried columns.\n",
      "\n",
      "## SQL Examples\n",
      "\n",
      "```sql\n",
      "-- 1. Create a new table with a primary key\n",
      "CREATE TABLE players (\n",
      "    player_id INT PRIMARY KEY AUTO_INCREMENT,\n",
      "    last_name VARCHAR(255),\n",
      "    first_name VARCHAR(255),\n",
      "    age INT,\n",
      "    G INT,\n",
      "    PA INT,\n",
      "    R INT,\n",
      "    H INT,\n",
      "    `2B` INT,\n",
      "    `3B` INT,\n",
      "    HR INT,\n",
      "    RBI INT,\n",
      "    SB INT,\n",
      "    CS INT,\n",
      "    BB INT,\n",
      "    SO INT\n",
      ");\n",
      "\n",
      "-- 2. Migrate data from the old table (assuming it's named 'baseball')\n",
      "INSERT INTO players (last_name, first_name, age, G, PA, R, H, `2B`, `3B`, HR, RBI, SB, CS, BB, SO)\n",
      "SELECT `Last`, `First`, Age, G, PA, R, H, `2B`, `3B`, HR, RBI, SB, CS, BB, SO\n",
      "FROM baseball;\n",
      "\n",
      "-- 3. Drop redundant columns (example: dropping 'AB' and 'H' after analysis)\n",
      "--  Before running this, ensure you understand the implications!\n",
      "-- ALTER TABLE players DROP COLUMN AB;\n",
      "-- ALTER TABLE players DROP COLUMN H;\n",
      "\n",
      "-- 4. Create indexes\n",
      "CREATE INDEX idx_last_name ON players (last_name);\n",
      "CREATE INDEX idx_first_name ON players (first_name);\n",
      "CREATE INDEX idx_age ON players (age);\n",
      "```\n",
      "\n",
      "## Entity Relationship Diagram\n",
      "\n",
      "The recommended data model is a single table:\n",
      "\n",
      "**players**\n",
      "\n",
      "*   player\\_id (INT, PRIMARY KEY, AUTO\\_INCREMENT)\n",
      "*   last\\_name (VARCHAR(255))\n",
      "*   first\\_name (VARCHAR(255))\n",
      "*   age (INT)\n",
      "*   G (INT)\n",
      "*   PA (INT)\n",
      "*   R (INT)\n",
      "*   2B (INT)\n",
      "*   3B (INT)\n",
      "*   HR (INT)\n",
      "*   RBI (INT)\n",
      "*   SB (INT)\n",
      "*   CS (INT)\n",
      "*   BB (INT)\n",
      "*   SO (INT)\n",
      "\n",
      "**Explanation:**\n",
      "\n",
      "*   The `player_id` is a surrogate primary key.\n",
      "*   The remaining columns represent player statistics.\n",
      "*   Indexes are added to `last_name`, `first_name`, and `age` for faster lookups.\n",
      "*   Redundant columns (AB and H) have been removed after correlation analysis.\n",
      "```\n"
     ]
    }
   ],
   "source": [
    "# With LLM\n",
    "result = restructure(\"./baseball.csv\", api_key= API_KEY, model=\"gemini-2.0-flash\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- SQL Script for Database Restructuring\n",
      "-- Generated by csvdiffgpt\n",
      "-- Original file: baseball.csv\n",
      "-- Total recommendations: 21\n",
      "\n",
      "-- IMPORTANT: Review and modify this script before execution\n",
      "-- All table names should be replaced with your actual table names\n",
      "-- Tables are referred to as 'your_table' by default\n",
      "\n",
      "-- Current table structure\n",
      "CREATE TABLE baseball (\n",
      "    Last VARCHAR(31) NOT NULL,\n",
      "    First VARCHAR(23) NOT NULL,\n",
      "    Age INTEGER NOT NULL,\n",
      "    G INTEGER NOT NULL,\n",
      "    PA INTEGER NOT NULL,\n",
      "    AB INTEGER NOT NULL,\n",
      "    R INTEGER NOT NULL,\n",
      "    H INTEGER NOT NULL,\n",
      "    2B INTEGER NOT NULL,\n",
      "    3B INTEGER NOT NULL,\n",
      "    HR INTEGER NOT NULL,\n",
      "    RBI INTEGER NOT NULL,\n",
      "    SB INTEGER NOT NULL,\n",
      "    CS INTEGER NOT NULL,\n",
      "    BB INTEGER NOT NULL,\n",
      "    SO INTEGER NOT NULL\n",
      ");\n",
      "\n",
      "-- REDUNDANCY CHANGES\n",
      "-- ==================================================\n",
      "\n",
      "-- Recommendation 1: Columns 'PA' and 'AB' are highly correlated (1.00), consider removing one\n",
      "-- Severity: medium\n",
      "\n",
      "-- Columns 'PA' and 'AB' are highly correlated (1.00)\n",
      "-- Evaluate whether to keep both\n",
      "-- ALTER TABLE your_table DROP COLUMN AB;  -- Only if redundant\n",
      "\n",
      "-- --------------------------------------------------\n",
      "\n",
      "-- Recommendation 2: Columns 'AB' and 'H' are highly correlated (0.99), consider removing one\n",
      "-- Severity: medium\n",
      "\n",
      "-- Columns 'AB' and 'H' are highly correlated (0.99)\n",
      "-- Evaluate whether to keep both\n",
      "-- ALTER TABLE your_table DROP COLUMN H;  -- Only if redundant\n",
      "\n",
      "-- --------------------------------------------------\n",
      "\n",
      "-- Recommendation 3: Columns 'PA' and 'H' are highly correlated (0.99), consider removing one\n",
      "-- Severity: medium\n",
      "\n",
      "-- Columns 'PA' and 'H' are highly correlated (0.99)\n",
      "-- Evaluate whether to keep both\n",
      "-- ALTER TABLE your_table DROP COLUMN H;  -- Only if redundant\n",
      "\n",
      "-- --------------------------------------------------\n",
      "\n",
      "-- Recommendation 4: Columns 'R' and 'H' are highly correlated (0.97), consider removing one\n",
      "-- Severity: medium\n",
      "\n",
      "-- Columns 'R' and 'H' are highly correlated (0.97)\n",
      "-- Evaluate whether to keep both\n",
      "-- ALTER TABLE your_table DROP COLUMN H;  -- Only if redundant\n",
      "\n",
      "-- --------------------------------------------------\n",
      "\n",
      "-- Recommendation 5: Columns 'PA' and 'R' are highly correlated (0.97), consider removing one\n",
      "-- Severity: medium\n",
      "\n",
      "-- Columns 'PA' and 'R' are highly correlated (0.97)\n",
      "-- Evaluate whether to keep both\n",
      "-- ALTER TABLE your_table DROP COLUMN R;  -- Only if redundant\n",
      "\n",
      "-- --------------------------------------------------\n",
      "\n",
      "-- Recommendation 6: Columns 'AB' and 'R' are highly correlated (0.97), consider removing one\n",
      "-- Severity: medium\n",
      "\n",
      "-- Columns 'AB' and 'R' are highly correlated (0.97)\n",
      "-- Evaluate whether to keep both\n",
      "-- ALTER TABLE your_table DROP COLUMN R;  -- Only if redundant\n",
      "\n",
      "-- --------------------------------------------------\n",
      "\n",
      "-- Recommendation 7: Columns 'H' and '2B' are highly correlated (0.96), consider removing one\n",
      "-- Severity: medium\n",
      "\n",
      "-- Columns 'H' and '2B' are highly correlated (0.96)\n",
      "-- Evaluate whether to keep both\n",
      "-- ALTER TABLE your_table DROP COLUMN 2B;  -- Only if redundant\n",
      "\n",
      "-- --------------------------------------------------\n",
      "\n",
      "-- Recommendation 8: Columns 'G' and 'PA' are correlated (0.95), but may contain unique information\n",
      "-- Severity: low\n",
      "\n",
      "-- Note: Columns 'G' and 'PA' are correlated (0.95)\n",
      "\n",
      "-- --------------------------------------------------\n",
      "\n",
      "-- Recommendation 9: Columns 'G' and 'AB' are correlated (0.95), but may contain unique information\n",
      "-- Severity: low\n",
      "\n",
      "-- Note: Columns 'G' and 'AB' are correlated (0.95)\n",
      "\n",
      "-- --------------------------------------------------\n",
      "\n",
      "-- Recommendation 10: Columns 'AB' and '2B' are correlated (0.94), but may contain unique information\n",
      "-- Severity: low\n",
      "\n",
      "-- Note: Columns 'AB' and '2B' are correlated (0.94)\n",
      "\n",
      "-- --------------------------------------------------\n",
      "\n",
      "-- Recommendation 11: Columns 'PA' and '2B' are correlated (0.94), but may contain unique information\n",
      "-- Severity: low\n",
      "\n",
      "-- Note: Columns 'PA' and '2B' are correlated (0.94)\n",
      "\n",
      "-- --------------------------------------------------\n",
      "\n",
      "-- Recommendation 12: Columns 'PA' and 'RBI' are correlated (0.94), but may contain unique information\n",
      "-- Severity: low\n",
      "\n",
      "-- Note: Columns 'PA' and 'RBI' are correlated (0.94)\n",
      "\n",
      "-- --------------------------------------------------\n",
      "\n",
      "-- Recommendation 13: Columns 'AB' and 'RBI' are correlated (0.93), but may contain unique information\n",
      "-- Severity: low\n",
      "\n",
      "-- Note: Columns 'AB' and 'RBI' are correlated (0.93)\n",
      "\n",
      "-- --------------------------------------------------\n",
      "\n",
      "-- Recommendation 14: Columns 'H' and 'RBI' are correlated (0.93), but may contain unique information\n",
      "-- Severity: low\n",
      "\n",
      "-- Note: Columns 'H' and 'RBI' are correlated (0.93)\n",
      "\n",
      "-- --------------------------------------------------\n",
      "\n",
      "-- Recommendation 15: Columns 'R' and '2B' are correlated (0.93), but may contain unique information\n",
      "-- Severity: low\n",
      "\n",
      "-- Note: Columns 'R' and '2B' are correlated (0.93)\n",
      "\n",
      "-- --------------------------------------------------\n",
      "\n",
      "-- Recommendation 16: Columns 'G' and 'H' are correlated (0.93), but may contain unique information\n",
      "-- Severity: low\n",
      "\n",
      "-- Note: Columns 'G' and 'H' are correlated (0.93)\n",
      "\n",
      "-- --------------------------------------------------\n",
      "\n",
      "-- Recommendation 17: Columns 'R' and 'RBI' are correlated (0.93), but may contain unique information\n",
      "-- Severity: low\n",
      "\n",
      "-- Note: Columns 'R' and 'RBI' are correlated (0.93)\n",
      "\n",
      "-- --------------------------------------------------\n",
      "\n",
      "-- Recommendation 18: Columns 'HR' and 'RBI' are correlated (0.92), but may contain unique information\n",
      "-- Severity: low\n",
      "\n",
      "-- Note: Columns 'HR' and 'RBI' are correlated (0.92)\n",
      "\n",
      "-- --------------------------------------------------\n",
      "\n",
      "-- Recommendation 19: Columns '2B' and 'RBI' are correlated (0.91), but may contain unique information\n",
      "-- Severity: low\n",
      "\n",
      "-- Note: Columns '2B' and 'RBI' are correlated (0.91)\n",
      "\n",
      "-- --------------------------------------------------\n",
      "\n",
      "-- Recommendation 20: Columns 'G' and 'R' are correlated (0.91), but may contain unique information\n",
      "-- Severity: low\n",
      "\n",
      "-- Note: Columns 'G' and 'R' are correlated (0.91)\n",
      "\n",
      "-- --------------------------------------------------\n",
      "\n",
      "-- Recommendation 21: Columns 'PA' and 'BB' are correlated (0.90), but may contain unique information\n",
      "-- Severity: low\n",
      "\n",
      "-- Note: Columns 'PA' and 'BB' are correlated (0.90)\n",
      "\n",
      "-- --------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Without LLM\n",
    "result = restructure(\"./baseball.csv\", use_llm=False, format=\"sql\")\n",
    "print(result[\"output_code\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
